# Demo 4 - Evaluate your prototype with Prompt Flow

The goal of this demo is to evaluate the performance of the prototype you built in the [previous demo](./demo3_add_your_own_data_w_pf.md). Prompt Flow provides the ability to use AI-assisted evaluation tools to score the **generation quality** of your model. A common issue with generative AI models is that there's no ground truth to compare the generated text against, because they are non-deterministic. 

Prompt Flow helps you to overcome this limitation by instructing a second model of your choice - you are going to use *gpt-4* - to evaluate the input-output pairs generated by your flow, against pre-defined metrics.

## Evaluate the performance of your flow

The first thing you need to evaluate your flow is a test dataset. You can use the [sample dataset](./data/test_dataset.jsonl) provided in the data folder of this repository. 
From the flow you built in the previous demo, click on the *Evaluate* button and then select **Automated evaluation**.
![Evaluate button](./media/evaluate_button.png)